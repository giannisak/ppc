Fine-Tuning Large Language Models (LLMs) for Proposal Writing

Overview
This repository contains resources and code for fine-tuning large language models (LLMs) specifically for the task of proposal writing.
The goal is to enhance the model's ability to generate coherent and relevant proposal sections based on provided abstracts and subsections.

Key Features
On-Premises Deployment: Ensures data privacy, customization, reduced latency, and cost savings.
Model Specifications: Utilizes Phi-3 models by Microsoft, optimized for performance with various parameter and context window sizes.
Data Handling: Involves comprehensive preprocessing and preparation of proposal data, including cleaning, sectioning, and template formatting.
Fine-Tuning Process: Configures the model with LoRA, applies quantization for memory optimization, and defines training parameters.
Inference Capabilities: The fine-tuned model can generate proposal content tailored to specific consortium needs.

Next Steps
Experiment with different fine-tuning parameters and quantization techniques.
Adjust hyperparameters and test various model sizes.
Compare fine-tuned models with base models across benchmarks.
